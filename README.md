# ğŸ§  Personality-Driven Experiences: ML Classification Case Study

[![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![Machine Learning](https://img.shields.io/badge/ML-Classification-green.svg)]()
[![Status](https://img.shields.io/badge/Status-Complete-success.svg)]()

> Building intelligent personality inference through behavioral data to power hyper-personalized user experiences

## ğŸ“‹ Project Overview

**ConnectSphere**, a social-wellness platform, aims to revolutionize user engagement through personality-driven personalization. This project develops a robust machine learning classifier that accurately infers whether users are **Introverts** or **Extroverts** based solely on their in-app behavioral patternsâ€”no explicit quizzes required.

### Business Impact
- ğŸ¯ Personalized activity recommendations
- ğŸ’¡ Tailored engagement nudges
- ğŸ“ˆ Improved user retention
- ğŸ’° Enhanced premium conversion rates

## ğŸ“ Research Context

**Researcher:** Junaid  
**Role:** Research Scholar - Data Science

## ğŸ“Š Dataset Description

The dataset captures daily behavioral metrics for each user:

| Feature | Type | Description |
|---------|------|-------------|
| `Time_spent_Alone` | Numerical | Hours per day spent in solitary activities |
| `Comfort_Level_with_new_situations` | Categorical | Comfort with new features (Yes/No) |
| `Weekly_Social_Events_Attended` | Numerical | Number of social events per week |
| `Days_Going_Outdoors` | Numerical | Days going outdoors per week |
| `Energy_After_Socializing` | Categorical | Post-socialization energy (Drained: Yes/No) |
| `Size_of_Close-Friend_Circle` | Numerical | Size of close friend network |
| `Weekly_Social-Media_Posts` | Numerical | Social media posts per week |
| **Personality** | **Target** | **Introvert / Extrovert** |

**Dataset Size:** 2,900 user records (1,491 Extroverts, 1,409 Introverts)

## ğŸ” Methodology

### 1. Data Preprocessing
- **Missing Value Handling:** Mode imputation for categorical, median for numerical features
- **Class Balance:** Relatively balanced distribution (no resampling needed)
- **Duplicate Removal:** Eliminated redundant records
- **Outlier Analysis:** Visual inspection via boxplots (no significant outliers detected)

### 2. Statistical Analysis
- **Normality Testing:** Shapiro-Wilk test (all features non-normal, p < 0.05)
- **Variance Equality:** Levene's test (unequal variances across groups)
- **Non-Parametric Tests:**
  - Mann-Whitney U Test: All numerical features significantly different between groups (p < 1e-250)
  - Chi-Squared Test: Strong associations for categorical features (p â‰ˆ 0.0)

### 3. Feature Engineering
Created advanced features to capture complex behavioral patterns:

- **Transformations:**
  - `Post_frequency_log`: Log transformation for skewness handling
  - `Time_Spent_Alone_Squared`: Polynomial feature for non-linearity
  - `Post_Frequency_Cubed`: Higher-order polynomial term

- **Interaction Terms:**
  - `GoingOut_Friends_Interaction`: Outdoor activity Ã— Friend circle size
  - `Social_energy_gap`: Friend circle size - Time spent alone
  - `Activity_level`: Social events + Days going outdoors

**Feature Selection:** Removed `Social_Alone_Interaction` due to low correlation (0.027)

### 4. Model Development

Evaluated four classification algorithms:

| Model | Type |
|-------|------|
| Logistic Regression | Linear probabilistic classifier |
| K-Nearest Neighbors | Instance-based learner |
| Decision Tree | Rule-based classifier |
| Random Forest | Ensemble method |

## ğŸ“ˆ Results

### Baseline Performance (Pre-Tuning)

| Model | Accuracy | Precision | Recall | F1-Score | AUC-ROC |
|-------|----------|-----------|--------|----------|---------|
| Logistic Regression | 91.87% | 90.09% | 91.74% | 90.91% | 0.9186 |
| KNN | 91.06% | 88.50% | 91.74% | 90.09% | 0.9113 |
| Decision Tree | 84.96% | 83.33% | 82.57% | 82.95% | 0.8472 |
| Random Forest | 89.00% | 89.00% | 89.00% | 89.00% | N/A |

### Optimized Performance (Post-Tuning with GridSearchCV)

| Model | Accuracy | Precision | Recall | F1-Score | AUC-ROC |
|-------|----------|-----------|--------|----------|---------|
| **Random Forest** ğŸ† | **92.28%** | **89.47%** | **93.58%** | **91.48%** | **0.9493** |
| Logistic Regression | 92.68% | 90.27% | 93.58% | 91.89% | 0.9187 |
| KNN | 92.68% | 90.27% | 93.58% | 91.89% | 0.9360 |
| Decision Tree | 92.68% | 90.27% | 93.58% | 91.89% | 0.9444 |

### ğŸ¯ Key Findings

**Best Model:** Random Forest achieved the highest AUC-ROC (0.9493), indicating superior discrimination capability between personality types.

**Performance Gains:**
- Decision Tree: +7.72% accuracy improvement (largest gain)
- Random Forest: +3.28% accuracy, +4.58% recall improvement
- All models achieved >92% accuracy after tuning

## ğŸš€ Business Applications

1. **Activity Suggestion Engine**
   - Low-intensity activities for introverts
   - Group events for extroverts

2. **Smart Engagement Nudges**
   - Outdoor activity prompts for low-activity introverts
   - Social event recommendations for extroverts

3. **Premium Upsell Opportunities**
   - Personality-aligned feature recommendations
   - Customized subscription benefits

## ğŸ› ï¸ Technical Stack

- **Language:** Python 3.8+
- **ML Libraries:** scikit-learn, pandas, numpy
- **Visualization:** matplotlib, seaborn
- **Statistical Analysis:** scipy
- **Model Optimization:** GridSearchCV

## ğŸ“ Project Structure

```
personality-classification/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                  # Original dataset
â”‚   â””â”€â”€ processed/            # Cleaned and engineered features
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ 01_eda.ipynb         # Exploratory data analysis
â”‚   â”œâ”€â”€ 02_preprocessing.ipynb
â”‚   â””â”€â”€ 03_modeling.ipynb
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ preprocessing.py
â”‚   â”œâ”€â”€ feature_engineering.py
â”‚   â””â”€â”€ model_training.py
â”œâ”€â”€ results/
â”‚   â”œâ”€â”€ figures/             # Plots and visualizations
â”‚   â””â”€â”€ models/              # Saved model artifacts
â”œâ”€â”€ docs/
â”‚   â””â”€â”€ classification_personality.pdf
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

## ğŸ”¬ Key Insights

1. **All behavioral features significantly correlate with personality type** (Mann-Whitney U test, p < 1e-250)
2. **Categorical features show perfect dependency** with personality (Chi-squared, p â‰ˆ 0.0)
3. **Feature engineering boosted model performance** by capturing non-linear patterns
4. **Ensemble methods outperformed** individual classifiers post-tuning
5. **High recall (93.58%)** ensures most introverts/extroverts are correctly identified

## ğŸ“ Statistical Rigor

- Verified non-normal distributions (Shapiro-Wilk test)
- Confirmed unequal variances (Levene's test)
- Applied appropriate non-parametric tests
- Checked multicollinearity (VIF analysis)
- Used robust imputation strategies (median/mode)

## ğŸ“ License

This project is part of an academic case study for educational purposes.

## ğŸ‘¤ Author

**Junaid**  
Research Scholar - Data Science

---

<p align="center">
  <i>Transforming behavioral insights into personalized experiences ğŸ¯</i>
</p>
